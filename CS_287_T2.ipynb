{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zPWH7XNO8nZM"
   },
   "source": [
    "# HW 2: Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fncLvGe28nZN"
   },
   "source": [
    "In this homework you will be building several varieties of language models.\n",
    "\n",
    "## Goal\n",
    "\n",
    "We ask that you construct the following models in Torch / NamedTensor:\n",
    "\n",
    "1. A count-based trigram model with linear-interpolation. $$p(y_t | y_{1:t-1}) =  \\alpha_1 p(y_t | y_{t-2}, y_{t-1}) + \\alpha_2 p(y_t | y_{t-1}) + (1 - \\alpha_1 - \\alpha_2) p(y_t) $$\n",
    "2. A neural network language model (consult *A Neural Probabilistic Language Model* http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)\n",
    "3. An LSTM language model (consult *Recurrent Neural Network Regularization*, https://arxiv.org/pdf/1409.2329.pdf) \n",
    "4. Your own extensions to these models.\n",
    "\n",
    "\n",
    "Consult the papers provided for hyperparameters.\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TxPRHeF08nZO"
   },
   "source": [
    "## Setup\n",
    "\n",
    "This notebook provides a working definition of the setup of the problem itself. You may construct your models inline or use an external setup (preferred) to build your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4nqdDeot8nZP"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchtext.vocab import Vectors\n",
    "\n",
    "from namedtensor import ntorch, NamedTensor\n",
    "from namedtensor.text import NamedField\n",
    "\n",
    "from load_data import load_text\n",
    "from models import TrigramModel, LSTM\n",
    "from train_models import make_kaggle_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, val_iter, test_iter, TEXT = load_text(\"./data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TrigramModel(.8, .16, len(TEXT.vocab))\n",
    "model.fit(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_kaggle_submission(model, TEXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/wiki.simple.vec: 293MB [03:07, 1.56MB/s]                              \n",
      "  0%|          | 0/111051 [00:00<?, ?it/s]Skipping token b'111051' with 1-dimensional vector [b'300']; likely a header\n",
      " 99%|█████████▉| 109946/111051 [00:09<00:00, 10917.56it/s]\n"
     ]
    }
   ],
   "source": [
    "# Build the vocabulary with word embeddings\n",
    "TEXT.vocab.load_vectors('fasttext.simple.300d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(ntorch.nn.Module):\n",
    "    def __init__(self, TEXT, hidden_size=50, layers=1, dropout = 0.2, device = 'cpu'):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.TEXT = TEXT\n",
    "        self.pretrained_emb = TEXT.vocab.vectors.to(device)\n",
    "        self.embedding = ntorch.nn.Embedding.from_pretrained(self.pretrained_emb, freeze=True)\n",
    "        self.lstm = ntorch.nn.LSTM(self.pretrained_emb.shape[1], hidden_size, bidirectional=True).spec(\"embedding\", \"seqlen\", \"lstm\")\n",
    "        self.lstm_dropout = ntorch.nn.Dropout(dropout)\n",
    "        self.linear = ntorch.nn.Linear(2*hidden_size, len(TEXT.vocab.itos)).spec('lstm', 'out')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.lstm_dropout(x)\n",
    "        x = self.linear(x)\n",
    "        return x  \n",
    "\n",
    "    def fit(self, train_iter, lr = 1e-2, momentum = 0.9, batch_size = 128, epochs = 10, interval = 1):\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.SGD(self.parameters(), lr=lr, momentum=momentum)\n",
    "        train_iter.batch_size = batch_size\n",
    "\n",
    "        for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "\n",
    "            running_loss = 0.0\n",
    "            for i, data in enumerate(train_iter, 0):\n",
    "                # get the inputs\n",
    "                inputs, labels = data.text, data.target\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward + backward + optimize\n",
    "                outputs = self(inputs)\n",
    "                loss = criterion(\n",
    "                    outputs.transpose('batch', 'out', 'seqlen').values, \n",
    "                    labels.transpose('batch','seqlen').values\n",
    "                )\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # print statistics\n",
    "                running_loss += loss.item()\n",
    "                if i % interval == interval-1:    # print every 2000 mini-batches\n",
    "                    print('[epoch: {}, batch: {}] loss: {}'.format(epoch + 1, i + 1, running_loss / interval))\n",
    "                running_loss = 0.0\n",
    "\n",
    "        print('Finished Training')\n",
    "\n",
    "    def predict(self, text, predict_last = False):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 1, batch: 1] loss: 9.213565826416016\n",
      "[epoch: 1, batch: 2] loss: 9.193037033081055\n",
      "[epoch: 1, batch: 3] loss: 9.151989936828613\n",
      "[epoch: 1, batch: 4] loss: 9.087571144104004\n",
      "[epoch: 1, batch: 5] loss: 9.003547668457031\n",
      "[epoch: 1, batch: 6] loss: 8.880457878112793\n",
      "[epoch: 1, batch: 7] loss: 8.683112144470215\n",
      "[epoch: 1, batch: 8] loss: 8.439841270446777\n",
      "[epoch: 1, batch: 9] loss: 8.09653091430664\n",
      "[epoch: 1, batch: 10] loss: 7.86899471282959\n",
      "[epoch: 1, batch: 11] loss: 7.858415603637695\n",
      "[epoch: 1, batch: 12] loss: 7.57770299911499\n",
      "[epoch: 1, batch: 13] loss: 7.568233013153076\n",
      "[epoch: 1, batch: 14] loss: 7.561644077301025\n",
      "[epoch: 1, batch: 15] loss: 7.583150863647461\n",
      "[epoch: 1, batch: 16] loss: 7.524545192718506\n",
      "[epoch: 1, batch: 17] loss: 7.360491752624512\n",
      "[epoch: 1, batch: 18] loss: 7.307711601257324\n",
      "[epoch: 1, batch: 19] loss: 7.2943243980407715\n",
      "[epoch: 1, batch: 20] loss: 7.205414295196533\n",
      "[epoch: 1, batch: 21] loss: 7.203909397125244\n",
      "[epoch: 1, batch: 22] loss: 7.111798286437988\n",
      "[epoch: 1, batch: 23] loss: 7.052992820739746\n",
      "[epoch: 1, batch: 24] loss: 7.074848651885986\n",
      "[epoch: 1, batch: 25] loss: 7.07115364074707\n",
      "[epoch: 1, batch: 26] loss: 6.914303779602051\n",
      "[epoch: 1, batch: 27] loss: 7.008279323577881\n",
      "[epoch: 1, batch: 28] loss: 6.997671604156494\n",
      "[epoch: 1, batch: 29] loss: 6.914183616638184\n",
      "[epoch: 1, batch: 30] loss: 6.886202335357666\n",
      "[epoch: 1, batch: 31] loss: 6.785632133483887\n",
      "[epoch: 1, batch: 32] loss: 6.77689790725708\n",
      "[epoch: 1, batch: 33] loss: 6.862945556640625\n",
      "[epoch: 1, batch: 34] loss: 6.842432498931885\n",
      "[epoch: 1, batch: 35] loss: 6.836973190307617\n",
      "[epoch: 1, batch: 36] loss: 6.782364845275879\n",
      "[epoch: 1, batch: 37] loss: 6.739480495452881\n",
      "[epoch: 1, batch: 38] loss: 6.75173282623291\n",
      "[epoch: 1, batch: 39] loss: 6.626376152038574\n",
      "[epoch: 1, batch: 40] loss: 6.598277568817139\n",
      "[epoch: 1, batch: 41] loss: 6.572939395904541\n",
      "[epoch: 1, batch: 42] loss: 6.538065433502197\n",
      "[epoch: 1, batch: 43] loss: 6.543356418609619\n",
      "[epoch: 1, batch: 44] loss: 6.44503116607666\n",
      "[epoch: 1, batch: 45] loss: 6.3906402587890625\n",
      "[epoch: 1, batch: 46] loss: 6.395657539367676\n",
      "[epoch: 1, batch: 47] loss: 6.294557571411133\n",
      "[epoch: 1, batch: 48] loss: 6.3145527839660645\n",
      "[epoch: 1, batch: 49] loss: 6.226107120513916\n",
      "[epoch: 1, batch: 50] loss: 6.172308921813965\n",
      "[epoch: 1, batch: 51] loss: 6.177148818969727\n",
      "[epoch: 1, batch: 52] loss: 6.020622253417969\n",
      "[epoch: 1, batch: 53] loss: 6.075048446655273\n",
      "[epoch: 1, batch: 54] loss: 5.942563056945801\n",
      "[epoch: 1, batch: 55] loss: 5.967493534088135\n",
      "[epoch: 1, batch: 56] loss: 5.953201770782471\n",
      "[epoch: 1, batch: 57] loss: 5.894665241241455\n",
      "[epoch: 1, batch: 58] loss: 5.877040386199951\n",
      "[epoch: 1, batch: 59] loss: 5.8392252922058105\n",
      "[epoch: 1, batch: 60] loss: 5.814469337463379\n",
      "[epoch: 1, batch: 61] loss: 5.773831844329834\n",
      "[epoch: 1, batch: 62] loss: 5.720544815063477\n",
      "[epoch: 1, batch: 63] loss: 5.648543834686279\n",
      "[epoch: 1, batch: 64] loss: 5.744832992553711\n",
      "[epoch: 1, batch: 65] loss: 5.66242790222168\n",
      "[epoch: 1, batch: 66] loss: 5.585138320922852\n",
      "[epoch: 1, batch: 67] loss: 5.559024333953857\n",
      "[epoch: 1, batch: 68] loss: 5.596460342407227\n",
      "[epoch: 1, batch: 69] loss: 5.537334442138672\n",
      "[epoch: 1, batch: 70] loss: 5.543673992156982\n",
      "[epoch: 1, batch: 71] loss: 5.423729419708252\n",
      "[epoch: 1, batch: 72] loss: 5.49946928024292\n",
      "[epoch: 1, batch: 73] loss: 5.419661045074463\n",
      "[epoch: 1, batch: 74] loss: 5.400148868560791\n",
      "[epoch: 1, batch: 75] loss: 5.386295318603516\n",
      "[epoch: 1, batch: 76] loss: 5.321460723876953\n",
      "[epoch: 1, batch: 77] loss: 5.3442487716674805\n",
      "[epoch: 1, batch: 78] loss: 5.318991661071777\n",
      "[epoch: 1, batch: 79] loss: 5.273256301879883\n",
      "[epoch: 1, batch: 80] loss: 5.117183208465576\n",
      "[epoch: 1, batch: 81] loss: 5.139954566955566\n",
      "[epoch: 1, batch: 82] loss: 5.134158611297607\n",
      "[epoch: 1, batch: 83] loss: 5.241034507751465\n",
      "[epoch: 1, batch: 84] loss: 5.180600643157959\n",
      "[epoch: 1, batch: 85] loss: 5.156760215759277\n",
      "[epoch: 1, batch: 86] loss: 5.170530796051025\n",
      "[epoch: 1, batch: 87] loss: 5.254716396331787\n",
      "[epoch: 1, batch: 88] loss: 5.247218132019043\n",
      "[epoch: 1, batch: 89] loss: 5.194395065307617\n",
      "[epoch: 1, batch: 90] loss: 5.0922956466674805\n",
      "[epoch: 1, batch: 91] loss: 5.1113128662109375\n",
      "[epoch: 1, batch: 92] loss: 5.090708255767822\n",
      "[epoch: 1, batch: 93] loss: 5.028844833374023\n",
      "[epoch: 1, batch: 94] loss: 5.046383857727051\n",
      "[epoch: 1, batch: 95] loss: 5.053084373474121\n",
      "[epoch: 1, batch: 96] loss: 5.124346733093262\n",
      "[epoch: 1, batch: 97] loss: 5.010471343994141\n",
      "[epoch: 1, batch: 98] loss: 5.0461320877075195\n",
      "[epoch: 1, batch: 99] loss: 5.012378215789795\n",
      "[epoch: 1, batch: 100] loss: 4.929771900177002\n",
      "[epoch: 1, batch: 101] loss: 4.99587345123291\n",
      "[epoch: 1, batch: 102] loss: 4.850872039794922\n",
      "[epoch: 1, batch: 103] loss: 4.91158390045166\n",
      "[epoch: 1, batch: 104] loss: 4.922701835632324\n",
      "[epoch: 1, batch: 105] loss: 4.895265102386475\n",
      "[epoch: 1, batch: 106] loss: 4.889903545379639\n",
      "[epoch: 1, batch: 107] loss: 4.8875412940979\n",
      "[epoch: 1, batch: 108] loss: 4.935691833496094\n",
      "[epoch: 1, batch: 109] loss: 4.841991424560547\n",
      "[epoch: 1, batch: 110] loss: 4.744556903839111\n",
      "[epoch: 1, batch: 111] loss: 4.785680294036865\n",
      "[epoch: 1, batch: 112] loss: 4.671301364898682\n",
      "[epoch: 1, batch: 113] loss: 4.81640625\n",
      "[epoch: 1, batch: 114] loss: 4.739619731903076\n",
      "[epoch: 1, batch: 115] loss: 4.779871463775635\n",
      "[epoch: 1, batch: 116] loss: 4.639826774597168\n",
      "[epoch: 1, batch: 117] loss: 4.6260552406311035\n",
      "[epoch: 1, batch: 118] loss: 4.668643474578857\n",
      "[epoch: 1, batch: 119] loss: 4.692224025726318\n",
      "[epoch: 1, batch: 120] loss: 4.6101837158203125\n",
      "[epoch: 1, batch: 121] loss: 4.632543087005615\n",
      "[epoch: 1, batch: 122] loss: 4.5936479568481445\n",
      "[epoch: 1, batch: 123] loss: 4.639975547790527\n",
      "[epoch: 1, batch: 124] loss: 4.55247688293457\n",
      "[epoch: 1, batch: 125] loss: 4.671604633331299\n",
      "[epoch: 1, batch: 126] loss: 4.700003623962402\n",
      "[epoch: 1, batch: 127] loss: 4.668694972991943\n",
      "[epoch: 1, batch: 128] loss: 4.620806694030762\n",
      "[epoch: 1, batch: 129] loss: 4.658383846282959\n",
      "[epoch: 1, batch: 130] loss: 4.630982398986816\n",
      "[epoch: 1, batch: 131] loss: 4.643568515777588\n",
      "[epoch: 1, batch: 132] loss: 4.639170169830322\n",
      "[epoch: 1, batch: 133] loss: 4.603179454803467\n",
      "[epoch: 1, batch: 134] loss: 4.513616561889648\n",
      "[epoch: 1, batch: 135] loss: 4.531602382659912\n",
      "[epoch: 1, batch: 136] loss: 4.476464748382568\n",
      "[epoch: 1, batch: 137] loss: 4.532103538513184\n",
      "[epoch: 1, batch: 138] loss: 4.531488418579102\n",
      "[epoch: 1, batch: 139] loss: 4.451022624969482\n",
      "[epoch: 1, batch: 140] loss: 4.5758280754089355\n",
      "[epoch: 1, batch: 141] loss: 4.506751537322998\n",
      "[epoch: 1, batch: 142] loss: 4.521378040313721\n",
      "[epoch: 1, batch: 143] loss: 4.443611145019531\n",
      "[epoch: 1, batch: 144] loss: 4.3908514976501465\n",
      "[epoch: 1, batch: 145] loss: 4.375699043273926\n",
      "[epoch: 1, batch: 146] loss: 4.482264518737793\n",
      "[epoch: 1, batch: 147] loss: 4.305303573608398\n",
      "[epoch: 1, batch: 148] loss: 4.361715793609619\n",
      "[epoch: 1, batch: 149] loss: 4.388120174407959\n",
      "[epoch: 1, batch: 150] loss: 4.467428207397461\n",
      "[epoch: 1, batch: 151] loss: 4.362813472747803\n",
      "[epoch: 1, batch: 152] loss: 4.393026828765869\n",
      "[epoch: 1, batch: 153] loss: 4.491487503051758\n",
      "[epoch: 1, batch: 154] loss: 4.4767937660217285\n",
      "[epoch: 1, batch: 155] loss: 4.377305030822754\n",
      "[epoch: 1, batch: 156] loss: 4.451157093048096\n",
      "[epoch: 1, batch: 157] loss: 4.377654552459717\n",
      "[epoch: 1, batch: 158] loss: 4.401749610900879\n",
      "[epoch: 1, batch: 159] loss: 4.401589393615723\n",
      "[epoch: 1, batch: 160] loss: 4.343122959136963\n",
      "[epoch: 1, batch: 161] loss: 4.304110050201416\n",
      "[epoch: 1, batch: 162] loss: 4.298811912536621\n",
      "[epoch: 1, batch: 163] loss: 4.243494033813477\n",
      "[epoch: 1, batch: 164] loss: 4.312282085418701\n",
      "[epoch: 1, batch: 165] loss: 4.329287528991699\n",
      "[epoch: 1, batch: 166] loss: 4.268074035644531\n",
      "[epoch: 1, batch: 167] loss: 4.194354057312012\n",
      "[epoch: 1, batch: 168] loss: 4.30093240737915\n",
      "[epoch: 1, batch: 169] loss: 4.235388278961182\n",
      "[epoch: 1, batch: 170] loss: 4.284234523773193\n",
      "[epoch: 1, batch: 171] loss: 4.226536273956299\n",
      "[epoch: 1, batch: 172] loss: 4.259661674499512\n",
      "[epoch: 1, batch: 173] loss: 4.330911159515381\n",
      "[epoch: 1, batch: 174] loss: 4.28954553604126\n",
      "[epoch: 1, batch: 175] loss: 4.165900230407715\n",
      "[epoch: 1, batch: 176] loss: 4.188664436340332\n",
      "[epoch: 1, batch: 177] loss: 4.171220779418945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 1, batch: 178] loss: 4.297300815582275\n",
      "[epoch: 1, batch: 179] loss: 4.225895881652832\n",
      "[epoch: 1, batch: 180] loss: 4.177648067474365\n",
      "[epoch: 1, batch: 181] loss: 4.1401448249816895\n",
      "[epoch: 1, batch: 182] loss: 4.17262077331543\n",
      "[epoch: 1, batch: 183] loss: 4.141529560089111\n",
      "[epoch: 1, batch: 184] loss: 4.212691307067871\n",
      "[epoch: 1, batch: 185] loss: 4.048442840576172\n",
      "[epoch: 1, batch: 186] loss: 4.012235164642334\n",
      "[epoch: 1, batch: 187] loss: 4.0375590324401855\n",
      "[epoch: 1, batch: 188] loss: 4.246653079986572\n",
      "[epoch: 1, batch: 189] loss: 4.302437782287598\n",
      "[epoch: 1, batch: 190] loss: 4.24398946762085\n",
      "[epoch: 1, batch: 191] loss: 4.106969833374023\n",
      "[epoch: 1, batch: 192] loss: 4.153709888458252\n",
      "[epoch: 1, batch: 193] loss: 4.127676486968994\n",
      "[epoch: 1, batch: 194] loss: 4.157070636749268\n",
      "[epoch: 1, batch: 195] loss: 4.242199420928955\n",
      "[epoch: 1, batch: 196] loss: 4.081156253814697\n",
      "[epoch: 1, batch: 197] loss: 4.167463779449463\n",
      "[epoch: 1, batch: 198] loss: 4.115771770477295\n",
      "[epoch: 1, batch: 199] loss: 4.2268500328063965\n",
      "[epoch: 1, batch: 200] loss: 4.206946849822998\n",
      "[epoch: 1, batch: 201] loss: 4.120973587036133\n",
      "[epoch: 1, batch: 202] loss: 4.101177215576172\n",
      "[epoch: 1, batch: 203] loss: 4.1522746086120605\n",
      "[epoch: 1, batch: 204] loss: 4.079497814178467\n",
      "[epoch: 1, batch: 205] loss: 3.9464690685272217\n",
      "[epoch: 1, batch: 206] loss: 4.103975296020508\n",
      "[epoch: 1, batch: 207] loss: 4.181739330291748\n",
      "[epoch: 1, batch: 208] loss: 4.122747898101807\n",
      "[epoch: 1, batch: 209] loss: 4.002025604248047\n",
      "[epoch: 1, batch: 210] loss: 4.04328727722168\n",
      "[epoch: 1, batch: 211] loss: 4.018187046051025\n",
      "[epoch: 1, batch: 212] loss: 4.049724578857422\n",
      "[epoch: 1, batch: 213] loss: 4.017677307128906\n",
      "[epoch: 1, batch: 214] loss: 4.016417980194092\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-165-df53b2ed2620>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTEXT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-163-f58f0e29ba15>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_iter, lr, momentum, batch_size, epochs, interval)\u001b[0m\n\u001b[1;32m     37\u001b[0m                     \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'batch'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'seqlen'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 )\n\u001b[0;32m---> 39\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "net = LSTM(TEXT)\n",
    "net.fit(train_iter, lr=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CS 287 T2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
